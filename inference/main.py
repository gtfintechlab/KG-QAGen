import json
import langchain
import logging
import os
import pandas as pd
import re
import sys
import time

from bs4 import BeautifulSoup
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.vectorstores.base import VectorStoreRetriever
from langchain_community.cache import InMemoryCache

import config

from model_loader import BaseModel

logging.basicConfig(level=logging.INFO)

langchain.llm_cache = InMemoryCache()


def clean_html(raw_html: str) -> str:
    """
    Minimal HTML-to-text cleaning with BeautifulSoup.
    Removes <script> and <style>, then collapses whitespace.
    """
    soup = BeautifulSoup(raw_html, "html.parser")
    for script in soup(["script", "style"]):
        script.decompose()
    text = soup.get_text(separator=" ")
    text = " ".join(text.split())
    return text


def chunk_text(doc_id: str, text: str, chunk_size: int):
    """
    Splits `text` into a list of substrings, each at most `chunk_size` characters.
    This is not for RAG, this is chunking in case of a large document.
    """
    if len(text) > chunk_size:
        logging.warning(
            f"Doc {doc_id} length {len(text)} > {chunk_size}, chunking..."
        )
        chunks = [text[i: i + chunk_size]
                  for i in range(0, len(text), chunk_size)]
        logging.info(
            f"Doc {doc_id} chunked into {len(chunks)} parts. Each up to {chunk_size} chars."
        )
        return chunks
    else:
        logging.info(
            f"Doc {doc_id} loaded, length={len(text)} chars."
        )
        return [text]


def load_document_text(doc_id: str, testing_regime: str = 'FULL') -> list[str]:
    """
    Load raw HTML and clean it, or load correct pieces for Oracle baseline.
    If longer than config.MAX_CHAR_FOR_SYSTEM, chunk it; else return as single chunk.
    Returns a list of chunks (one or more).
    """
    html_path = os.path.join(config.HTML_PATH, f'{doc_id}{".html" if testing_regime == "FULL" else "_gold.txt"}')
    if not os.path.exists(html_path):
        logging.error(f"HTML file not found: {html_path}")
        return []

    try:
        with open(html_path, 'r', encoding='utf-8') as file:
            html_data = file.read()
        
        if testing_regime == 'GOLD':
            return html_data

        # FULL
        cleaned = clean_html(html_data)
        if not cleaned:
            logging.warning(f"Doc {doc_id} is empty after cleaning.")
            return []
        return chunk_text(doc_id, cleaned, config.MAX_CHAR_FOR_SYSTEM)
        
    except Exception as e:
        logging.error(f"Error reading {json_path}: {e}")
        return []


def load_vector_db_text(vector_store, doc_id: str, question: str):
    """
    Extracting relevant chunks from a vector store for RAG baseline
    """
    retriever: VectorStoreRetriever = vector_store.as_retriever(
        search_kwargs={"k": config.RAG_TOP_K, "filter": {"docID": str(doc_id)}}
    )
    results = retriever.get_relevant_documents(question)
    result = "".join([doc.page_content for doc in results])
    return result


def build_RAG_prompt(document_text: str, question: str) -> list[dict]:
    user_instructions = (
        "[SYSTEM INPUT]\n"
        "You are a financial expert, and your task is to answer "
        "the question given to you based on the chunks of a credit agreement provided to you. "
        "If you believe the answer is not present among the chunks, say 'Not found'.\n\n"

        "[EXPECTED OUTPUT]\n"
        "Respond ONLY with the answer to your question, nothing else. See the example below.\n\n"

        "The given document:\n"
        "Apple Inc. is a technology company headquartered in Cupertino, California. "
        "It was founded by Steve Jobs, Steve Wozniak, and Ronald Wayne in 1976.\n\n"

        "The given question:\n"
        "Where is the headquarters of Apple Inc.?\n\n"

        "The expected output:\n"
        "Cupertino, California\n\n"

        "[USER INPUT]\n"
        f"Merged chunks:\n{document_text}\n\n"

        "[QUESTION]\n"
        f"{question}\n"
    )

    return [{"role": "user", "content": user_instructions}]


def build_GOLD_prompt(document_text: str, question: str) -> list[dict]:
    user_instructions = (
        "[SYSTEM INPUT]\n"
        "You are a financial expert, and your task is to answer "
        "the question given to you based on the pieces of a credit agreement which are guaranteed to contain the answer. "
        "If you still believe the answer is not present among the chunks, say 'Not found'.\n\n"

        "[EXPECTED OUTPUT]\n"
        "Respond ONLY with the answer to your question, nothing else. See the example below.\n\n"

        "The given document:\n"
        "Apple Inc. is a technology company headquartered in Cupertino, California. "
        "It was founded by Steve Jobs, Steve Wozniak, and Ronald Wayne in 1976.\n\n"

        "The given question:\n"
        "Where is the headquarters of Apple Inc.?\n\n"

        "The expected output:\n"
        "Cupertino, California\n\n"

        "[USER INPUT]\n"
        f"Merged chunks:\n{document_text}\n\n"

        "[QUESTION]\n"
        f"{question}\n"
    )

    return [{"role": "user", "content": user_instructions}]


def build_prompt_single(document_text: str, question: str, question_index: int) -> list[dict]:
    """
    Single user message combining instructions + doc text + question.
    Enforce returning only JSON.
    """
    user_instructions = (
        "[SYSTEM INPUT]\n"
        "You are a financial expert, and your task is to answer "
        "the question given to you about the provided credit agreement. "
        "If you believe the answer is not present in the agreement, say 'Not found'.\n\n"

        "[EXPECTED OUTPUT]\n"
        "Respond ONLY with valid JSON, nothing else. See the example below.\n\n"

        "The given document:\n"
        "Apple Inc. is a technology company headquartered in Cupertino, California. "
        "It was founded by Steve Jobs, Steve Wozniak, and Ronald Wayne in 1976.\n\n"

        "The given question:\n"
        "Q1: Where is the headquarters of Apple Inc.?\n\n"

        "The expected output:\n"
        "{\n"
        '  "answers": [\n'
        '    {"question_index": 1, "answer": "Cupertino, California"}\n'
        "  ]\n"
        "}\n\n"

        "[USER INPUT]\n"
        f"Document:\n{document_text}\n\n"

        "[QUESTION]\n"
        f"Q{question_index}: {question}\n"
    )

    return [{"role": "user", "content": user_instructions}]


def build_prompt_batch(document_text: str, questions: list[str]) -> list[dict]:
    """
    Constructs a single user message that includes multiple questions about a credit agreement.
    The response must be in valid JSON format only.
    """
    prompt_lines = [
        "[SYSTEM INPUT]\n"
        "You are a financial expert, and your task is to answer "
        "the questions given to you in batches about the provided credit agreement. "
        "If you believe the answer is not present in the agreement, say 'Not found'.\n\n"

        "[EXPECTED OUTPUT]\n"
        "Respond ONLY with valid JSON, nothing else. See the example below.\n\n"

        "The given document:\n"
        "Tesla, Inc. is an American electric vehicle and clean energy company founded in 2003 by Martin Eberhard and Marc Tarpenning. "
        "Elon Musk became the largest investor and later CEO.\n\n"

        "The given questions:\n"
        "Q1: Who founded Tesla?\n"
        "Q2: What year was Tesla founded?\n\n"

        "The expected output:\n"
        "{\n"
        '  "answers": [\n'
        '    {"question_index": 1, "answer": "Martin Eberhard, Marc Tarpenning"},\n'
        '    {"question_index": 2, "answer": "2003"}\n'
        "  ]\n"
        "}\n\n"

        "[USER INPUT]\n"
        f"Document:\n{document_text}\n\n"

        "[QUESTIONS]\n"
    ]

    for i, question in enumerate(questions, start=1):
        prompt_lines.append(f"Q{i}: {question}")

    combined_prompt = "\n".join(prompt_lines)

    return [{"role": "user", "content": combined_prompt}]


def build_prompt_combine_answers(partial_answers: list[str], questions: list[str]) -> list[dict]:
    """
    Build a prompt to merge/combine partial answers (in JSON form) from multiple chunks into final answers.
    We'll ask the LLM to output in the same JSON format:
    {
      "answers": [
        {"question_index": 1, "answer": "..."},
        ...
      ]
    }
    Enforce returning only JSON.
    """
    prompt_lines = [
        "[SYSTEM INPUT]\n"
        "You are a financial expert, and your task is to combine or merge "
        "the provided partial answers, coming from different chunks of a credit agreement, "
        "into a single final answer for each of the questions given to you. "
        "If you believe the answer is not present in the agreement, say 'Not found'.\n\n"

        "[EXPECTED OUTPUT]\n"

        "The given document:\n"
        "Tesla, Inc. is an American electric vehicle and clean energy company founded in 2003 by Martin Eberhard and Marc Tarpenning. "
        "Elon Musk became the largest investor and later CEO.\n\n"

        "The given questions:\n"
        "Q1: Who founded Tesla?\n"
        "Q2: What year was Tesla founded?\n\n"

        "The expected output:\n"
        "{\n"
        '  "answers": [\n'
        '    {"question_index": 1, "answer": "Martin Eberhard, Marc Tarpenning"},\n'
        '    {"question_index": 2, "answer": "2003"}\n'
        "  ]\n"
        "}\n\n"

        "[USER INPUT]\n"
    ]

    for i, ans in enumerate(partial_answers, start=1):
        prompt_lines.append(f"Chunk {i} partial answer JSON:\n{ans}\n")

    prompt_lines.append("\n[QUESTIONS]\n")
    for i, q in enumerate(questions, start=1):
        prompt_lines.append(f"Q{i}: {q}")

    combined_prompt = "\n".join(prompt_lines)

    return [{"role": "user", "content": combined_prompt}]


def parse_llm_json(raw_response: str, num_questions: int) -> dict:
    """
    Expects a JSON string, possibly embedded in extra text, like:

    Some preamble text...
    ```json
    {
      "answers": [
        {"question_index": 1, "answer": "..."},
        ...
      ]
    }
    ```
    Some additional commentary...

    Returns a dict: {1: "answer1", 2: "answer2", ...}
    If invalid JSON or missing fields, defaults to "LLM parse error".
    """
    default_result = {
        i: "LLM parse error" for i in range(1, num_questions + 1)}

    match = re.search(r"```json\s*(.*?)\s*```", raw_response, re.DOTALL)
    if match:
        cleaned_response = match.group(1).strip()
    else:
        cleaned_response = raw_response.strip()

    logging.debug(f"Parsing LLM JSON: {cleaned_response}")

    try:
        data = json.loads(cleaned_response)
        if "answers" not in data:
            logging.warning("No 'answers' key found in the JSON response.")
            return default_result

        answers = data["answers"]
        for ans in answers:
            idx = ans.get("question_index")
            content = ans.get("answer", "")
            if isinstance(idx, int) and 1 <= idx <= num_questions:
                default_result[idx] = content

        return default_result
    except json.JSONDecodeError as e:
        logging.warning(f"JSON parse error: {e}")
        return default_result


def parse_llm_list(raw_response: str, num_questions: int):
    default_result = {
        i: "LLM parse error" for i in range(1, num_questions + 1)}

    question_responses = raw_response.split("\n")
    for i in range(len(question_responses)):
        try:
            question_index = int(question_responses[i].split(':')[0][1:])
            question_response = question_responses[i].split(':')[1][1:]
            default_result[question_index] = question_response
        except:
            logging.warning(f'Issue parsing question {i}')

    return default_result


def call_llm_with_retries(llm, messages: list[dict], extra_log_info: str = "") -> str:
    """
    Call the LLM up to config.NUM_RETRIES times if blank is returned.
    We return the *raw string* from LLM (which should be JSON).
    extra_log_info can be used to log chunk/question context, etc.
    """
    for attempt in range(config.NUM_RETRIES):
        try:
            prompt_str = messages[0]['content'] if messages else ""
            logging.info(
                f"LLM call attempt {attempt+1}/{config.NUM_RETRIES} {extra_log_info} "
                f"(prompt length: {len(prompt_str)} chars)"
            )

            response = llm.invoke(messages)
            raw_output = response.content.strip() if hasattr(
                response, "content") else str(response).strip()
            print(raw_output)
            time.sleep(10.0)

            if raw_output:
                if config.WAIT_TIME_ENABLED:
                    time.sleep(config.WAIT_TIME_DURATION)
                return raw_output
            else:
                logging.warning(
                    f"Got an empty response from LLM. Retrying in 1s...")
                time.sleep(1.0)
        except Exception as e:
            print(raw_output)
            logging.error(f"LLM call error on attempt {attempt+1}: {e}")
            time.sleep(1.0)

    logging.error("All attempts returned empty response. Giving up.")
    return ""


def get_llm_json_response(llm, messages: list[dict], num_questions: int, extra_log_info: str) -> dict:
    """
    Attempts to get a valid JSON parse from the LLM.
    Retries multiple times (config.NUM_RETRIES) if the JSON parse fails.
    """
    parsed_result = {}
    for parse_attempt in range(config.NUM_RETRIES):
        raw_output = call_llm_with_retries(
            llm, messages, extra_log_info=extra_log_info)
        if not raw_output:
            logging.warning(
                f"Empty output from LLM (parse attempt {parse_attempt+1}). Retrying...")
            time.sleep(1.0)
            continue

        parsed_result = parse_llm_json(raw_output, num_questions)
        # Check if parse was successful
        if any(ans != "LLM parse error" for ans in parsed_result.values()):
            return parsed_result
        else:
            if config.MODEL_NAME != 'meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo':
                logging.warning(
                    f"Parse error (parse attempt {parse_attempt+1}). Retrying LLM call...")
                continue
            logging.warning(
                f"JSON parse error (parse attempt {parse_attempt+1}). Attempting alternative parsing...")
            parsed_result = parse_llm_list(raw_output, num_questions)
            if any(ans != "LLM parse error" for ans in parsed_result.values()):
                return parsed_result
            logging.warning(
                f"Parse error (parse attempt {parse_attempt+1}). Retrying LLM call...")

    return parsed_result


def main():
    # 1) Load the model
    model_loader = BaseModel(
        llm_provider=config.LLM_PROVIDER,
        model_name=config.MODEL_NAME,
        temperature=config.TEMPERATURE,
        max_tokens=config.max_tokens_generation
    )
    model_loader.load()
    llm = model_loader.get_model()
    if config.TESTING_REGIME == "RAG":
        embedding_model = HuggingFaceEmbeddings(model_name=config.RAG_MODEL)
        vector_store = FAISS.load_local(
            config.VECTOR_DB_DIR, embeddings=embedding_model,
            allow_dangerous_deserialization=True)

    # 2) Read the CSV
    input_csv_path = os.path.join(
        config.INPUT_PATH, f"{config.QUESTION_FILE}.csv")
    if not os.path.exists(input_csv_path):
        logging.error(f"Input CSV not found: {input_csv_path}")
        return

    df = pd.read_csv(input_csv_path)

    # Ensure we have an "llm_response" column to store results
    if "llm_response" not in df.columns:
        df["llm_response"] = ""

    # Group by document_number
    grouped = df.groupby("document_number")
    all_doc_ids = list(grouped.groups.keys())

    total_docs = len(all_doc_ids)
    processed_docs = 0
    chunked_docs = []

    logging.info(f"Total documents to process: {total_docs}")

    output_dir = config.OUTPUT_PATH
    os.makedirs(output_dir, exist_ok=True)

    def save_csv():
        """
        Saves the current DataFrame to CSV, then runs evaluation metrics.
        """
        # 4) Save results
        sanitized_model_name = config.MODEL_NAME.replace("/", "-")
        sanitized_model_name = re.sub(
            r'[<>:"/\\|?*]', '-', sanitized_model_name)

        output_csv = f"{config.QUESTION_FILE}_{sanitized_model_name}_{config.TESTING_REGIME}.csv"
        output_path = os.path.join(output_dir, output_csv)
        df.to_csv(output_path, index=False)
        logging.info(f"Saved LLM answers to {output_path}")

    try:
        # NEW OR CHANGED: We wrap the main doc-loop in a try/except
        for doc_id in all_doc_ids:
            processed_docs += 1
            logging.info(
                f"Processing document {processed_docs}/{total_docs} (doc_id={doc_id})...")

            group_indices = grouped.groups[doc_id]
            overall_indices_list = list(group_indices)

            question_batch_length = 50
            doc_chunks = load_document_text(
                str(doc_id), config.TESTING_REGIME)  # str for "GOLD"

            # If doc text is empty, mark all as 'No doc text'
            if not doc_chunks:
                logging.warning(
                    f"Document {doc_id} is empty. Setting llm_response='No doc text'.")
                for idx in overall_indices_list:
                    df.at[idx, "llm_response"] = "No doc text"
                # Save partial results after finishing each doc
                save_csv()
                continue

            # We process the doc's questions in sub-batches
            for q_start in range(0, len(overall_indices_list), question_batch_length):
                q_indices_list = overall_indices_list[q_start: q_start +
                                                      question_batch_length]
                questions = df.loc[q_indices_list, "question"].tolist()
                num_questions = len(questions)
                logging.info(
                    f"Processing {num_questions} questions for doc_id={doc_id}...")

                if config.TESTING_REGIME == "RAG":
                    for i, row_idx in enumerate(q_indices_list, start=1):
                        question = df.at[row_idx, "question"]
                        merged_chunks = load_vector_db_text(
                            vector_store, doc_id, question)
                        messages = build_RAG_prompt(merged_chunks, question)
                        if config.LLM_PROVIDER == "Custom":
                            response = llm.invoke(
                                messages, testing_regime=config.TESTING_REGIME)
                        else:
                            response = llm.invoke(messages)
                        df.at[row_idx, "llm_response"] = response.content.strip()
                    continue

                if config.TESTING_REGIME == "GOLD":
                    for i, row_idx in enumerate(q_indices_list, start=1):
                        question = df.at[row_idx, "question"]
                        messages = build_GOLD_prompt(doc_chunks, question)
                        if config.LLM_PROVIDER == "Custom":
                            response = llm.invoke(
                                messages, testing_regime=config.TESTING_REGIME)
                        else:
                            response = llm.invoke(messages)
                        df.at[row_idx, "llm_response"] = response.content.strip()
                    continue

                # If there's only 1 chunk, process it normally
                if len(doc_chunks) == 1:
                    single_chunk_text = doc_chunks[0]
                    if config.context_chat:
                        for i, row_idx in enumerate(q_indices_list, start=1):
                            question_text = df.at[row_idx, "question"]
                            log_msg = f"[doc={doc_id} chunk=1 question_index={i}]"
                            messages = build_prompt_single(
                                single_chunk_text, question_text, i)
                            parsed_answers = get_llm_json_response(
                                llm, messages, 1, extra_log_info=log_msg)
                            df.at[row_idx, "llm_response"] = parsed_answers[1]
                    else:
                        log_msg = f"[doc={doc_id} chunk=1 batch_mode]"
                        messages = build_prompt_batch(
                            single_chunk_text, questions)
                        parsed_answers = get_llm_json_response(
                            llm, messages, num_questions, extra_log_info=log_msg)
                        for i, row_idx in enumerate(q_indices_list, start=1):
                            df.at[row_idx, "llm_response"] = parsed_answers[i]

                else:
                    # Document required chunking, add to tracking
                    chunked_docs.append(doc_id)

                    # If multiple chunks
                    if config.context_chat:
                        # Each question is separate across all chunks
                        for i, row_idx in enumerate(q_indices_list, start=1):
                            question_text = df.at[row_idx, "question"]
                            partial_responses = []

                            for c_idx, chunk_text in enumerate(doc_chunks, start=1):
                                log_msg = f"[doc={doc_id} chunk={c_idx} question_index={i}]"
                                messages_chunk = build_prompt_single(
                                    chunk_text, question_text, i)
                                chunk_parsed_answers = get_llm_json_response(
                                    llm, messages_chunk, 1, extra_log_info=log_msg
                                )

                                if "LLM parse error" in chunk_parsed_answers[1]:
                                    partial_responses.append(
                                        '{"answers":[{"question_index":1,"answer":"LLM parse error"}]}')
                                else:
                                    partial_json_str = json.dumps({
                                        "answers": [
                                            {"question_index": 1,
                                                "answer": chunk_parsed_answers[1]}
                                        ]
                                    })
                                    partial_responses.append(partial_json_str)

                            combine_msg = f"[doc={doc_id} combine question_index={i}]"
                            combine_prompt = build_prompt_combine_answers(
                                partial_responses, [question_text])
                            combined_final = get_llm_json_response(
                                llm, combine_prompt, 1, extra_log_info=combine_msg)
                            df.at[row_idx, "llm_response"] = combined_final[1]

                    else:
                        # Batch mode across multiple chunks
                        partial_responses = []
                        for c_idx, chunk_text in enumerate(doc_chunks, start=1):
                            log_msg = f"[doc={doc_id} chunk={c_idx} batch_mode]"
                            messages_chunk = build_prompt_batch(
                                chunk_text, questions)
                            chunk_parsed_answers = get_llm_json_response(
                                llm, messages_chunk, num_questions, extra_log_info=log_msg
                            )

                            if any(ans == "LLM parse error" for ans in chunk_parsed_answers.values()):
                                fake_json = {
                                    "answers": [
                                        {"question_index": i,
                                            "answer": "LLM parse error"}
                                        for i in range(1, num_questions + 1)
                                    ]
                                }
                                partial_responses.append(json.dumps(fake_json))
                            else:
                                partial_json = {"answers": []}
                                for q_idx in range(1, num_questions + 1):
                                    partial_json["answers"].append(
                                        {"question_index": q_idx,
                                            "answer": chunk_parsed_answers[q_idx]}
                                    )
                                partial_responses.append(
                                    json.dumps(partial_json))

                        combine_msg = f"[doc={doc_id} combine batch_mode]"
                        combine_prompt = build_prompt_combine_answers(
                            partial_responses, questions)
                        combined_output = get_llm_json_response(
                            llm, combine_prompt, num_questions, extra_log_info=combine_msg)
                        for i, row_idx in enumerate(q_indices_list, start=1):
                            df.at[row_idx, "llm_response"] = combined_output[i]

            logging.info(
                f"Completed processing document {processed_docs}/{total_docs} (doc_id={doc_id}).")
            # NEW OR CHANGED: partial saving after each doc
            save_csv()

    except KeyboardInterrupt:
        # NEW OR CHANGED: If the user presses Ctrl+C or otherwise interrupts
        logging.warning(
            "Code terminated by user. Marking unprocessed documents with 'code terminated'...")

        # Mark all documents not processed yet as "code terminated"
        # processed_docs is the count of docs we already did
        # The ones we haven't started
        unprocessed_docs = all_doc_ids[processed_docs:]
        for udoc_id in unprocessed_docs:
            indices_ = grouped.groups[udoc_id]
            for idx in indices_:
                if df.at[idx, "llm_response"] == "":
                    df.at[idx, "llm_response"] = "code terminated"

        # Save final partial results
        save_csv()
        logging.warning("Partial results saved. Exiting now.")
        sys.exit(1)

    except Exception as e:
        # If there's an unexpected exception, log it
        logging.error(f"Unexpected top-level error: {e}", exc_info=True)

        # Mark all documents not processed as "code terminated"
        unprocessed_docs = all_doc_ids[processed_docs:]
        for udoc_id in unprocessed_docs:
            indices_ = grouped.groups[udoc_id]
            for idx in indices_:
                if df.at[idx, "llm_response"] == "":
                    df.at[idx, "llm_response"] = "code terminated"

        # Save partial results
        save_csv()
        logging.warning("Partial results saved. Exiting due to fatal error.")
        sys.exit(1)

    # If we complete everything without interruption:
    logging.info(
        f"Processing complete: {processed_docs}/{total_docs} documents processed successfully.")

    # 6) Summarize chunked docs
    if chunked_docs:
        unique_chunked = list(set(chunked_docs))
        logging.info(
            f"Documents that required chunking: {len(unique_chunked)}")
        logging.info(f"Chunked Document IDs: {unique_chunked}")
        print("\nDocuments that required chunking:", unique_chunked)
    else:
        logging.info("No documents required chunking.")


if __name__ == "__main__":
    main()
